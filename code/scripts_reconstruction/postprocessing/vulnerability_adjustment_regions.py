#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Aug 25 15:21:06 2020

@author: insauer
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
import numpy.ma as ma
from astropy.convolution import convolve
from scipy import stats

from pyts.decomposition import SingularSpectrumAnalysis


def runmean(data, halfwin):
    """
    Simple running mean.
    CAUTION: Data is *extended* at the edges by repeating the
    edge values; thereby any trend present in the data will
    become attenuated at the edges!
    """
    window = 2*halfwin + 1
    if window > len(data):
        print('Error: window too large!')
        import sys
        sys.exit(0)
    weights = np.repeat(1.0, window) / window
    #  Treat edges: Extend data
    extended_data = np.hstack([[data[0]] * (halfwin), data, [data[len(data)-1]]
                                * (halfwin)])
    # rm = np.convolve(extended_data, weights, 'valid')
    rm = convolve(extended_data, weights, boundary=None, nan_treatment='fill',
                  preserve_nan=False)
    return rm[halfwin:-halfwin]


def vul_func(ratio):
    """
    This functions estimates a vulnerability function, by flattening the ration
    of observed and modeled damages. Provided is a smoothing with different
    window-sizes for running means and a smoothing with the SSA tool. For
    further analysis only ssa_5 was considered (11-yr running mean)
    Parameters
    ----------
    ratio : Column of DataFrame
        Ratio of recorded to modeled damages

    Returns
    -------
    np.arrays
        Ratios with different window sizes

    """

    ratio11yr = ratio['ratios_D_Obs_D_CliExp'].rolling(window=11, min_periods=5, center=True).mean()

    ratio_reg = ratio['ratios_D_Obs_D_CliExp'].replace([np.nan], [ratio['ratios_D_Obs_D_CliExp'].median()])

    ratio_test_reg = np.zeros((1, 31))

    ratio_test_reg[0, :] = ratio_reg

    ssa = SingularSpectrumAnalysis(window_size=11, groups=None)
    X_ssa5 = ssa.fit_transform(ratio_test_reg)

    ssa_5 = X_ssa5[0, :]

    return ratio11yr, ssa_5


def get_rm(col_names, dataFrame, rm_window):
    """
    Function applies running mean on selected columns
    ----------
    col_names : string list
        Columns to be smoothed
    dataFrame : DataFrame
        DataFrame that contains columns to be smoothed
    rm_window : int
        window size for running mean

    Returns
    -------
    DataFrame
        DataFrame with smoothed columns

    """
    for col in col_names:
        dataFrame[col] = dataFrame[col].replace(
                                            [-np.inf, np.inf],
                                            [np.nan, np.nan])
        dataFrame[col] = runmean(dataFrame[col], rm_window)

    return dataFrame


def calc_cutoff(ratio):

    q30, q70 = np.nanpercentile(ratio, 30), np.nanpercentile(ratio, 70)

    iqr = q70 - q30

    cut_off = iqr * 5

    lower, upper = q30 - cut_off, q70 + cut_off

    return lower, upper


def pears_corr_obs(corr_obs_ts, corr_ts, use_log):
    """
    Pearson-Correlation of modeled and observed damages
    ----------
    corr_obs_ts : np.array
        observed damages
    corr_ts : np.array
        damages to be correlated
    use_log : string
        correlation in log space
    Returns
    -------
    CorrelationObject

    """
    if use_log:
        a = ma.masked_invalid(np.log10(corr_obs_ts).replace([-np.inf, np.inf],
                                                            [np.nan, np.nan]))
        b = ma.masked_invalid(np.log10(corr_ts))
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        # corrcoef = stats.spearmanr(a[msk], b[msk])

    else:
        a = ma.masked_invalid(corr_obs_ts.replace([-np.inf, np.inf],
                                                  [np.nan, np.nan]))
        b = ma.masked_invalid(corr_ts)
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        #  corrcoef = stats.spearmanr(a[msk], b[msk])

    return corrcoef


def rm_pears_corr_obs(corr_obs_ts, corr_ts, use_log):
    """
    Pearson-Correlation of modeled and observed damages, applying a running
    mean before (3yr)
    ----------
    corr_obs_ts : np.array
        observed damages
    corr_ts : np.array
        damages to be correlated
    use_log : string
        correlation in log space
    Returns
    -------
    CorrelationObject

    """
    rm_obs = runmean(np.array(corr_obs_ts), 1)
    rm_ts = runmean(np.array(corr_ts), 1)

    if use_log:
        a = ma.masked_invalid(np.log10(rm_obs).replace([-np.inf, np.inf],
                                                       [np.nan, np.nan]))
        b = ma.masked_invalid(np.log10(rm_ts))
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        #  corrcoef = stats.spearmanr(a[msk], b[msk])

    else:
        a = ma.masked_invalid(rm_obs)
        b = ma.masked_invalid(rm_ts)
        msk = (~a.mask & ~b.mask)
        corrcoef = ma.corrcoef(a[msk], b[msk])

        # corrcoef = stats.spearmanr(a[msk], b[msk])

    return corrcoef


def spear_corr_obs(corr_obs_ts, corr_ts, use_log):
    """
    Spearman-Rank-Correlation of modeled and observed damages, applying a
    running mean before (3yr)
    ----------
    corr_obs_ts : np.array
        observed damages
    corr_ts : np.array
        damages to be correlated
    use_log : string
        correlation in log space
    Returns
    -------
    CorrelationObject

    """
    if use_log:
        a = ma.masked_invalid(np.log10(corr_obs_ts).replace([-np.inf, np.inf],
                                                            [np.nan, np.nan]))
        b = ma.masked_invalid(np.log10(corr_ts))
        msk = (~a.mask & ~b.mask)

        tau, p_value = stats.kendalltau(a[msk], b[msk])

    else:
        a = ma.masked_invalid(corr_obs_ts.replace([-np.inf, np.inf],
                                                  [np.nan, np.nan]))
        b = ma.masked_invalid(corr_ts)
        msk = (~a.mask & ~b.mask)

        tau, p_value = stats.kendalltau(a[msk], b[msk])

    return tau, p_value


def adjust_dam(data, vul_func):
    """
    This function adjusts modeled damages for vulnerability changes using
    vulnerability functions
    ----------
    data : DataFrame
        DataFrame with modeled damages
    vul_func : np.array
        time dependent vulnerability function


    Returns
    -------
    np.arrays
        damages after accounting for vulnerability, including onethird and two-
        third model quantiles

    """
    vul_func = vul_func/vul_func.max()

    predicted_damages = data['D_CliExp_raw'] * vul_func
    predicted_damages = predicted_damages.replace([-np.inf, np.inf],
                                                  [np.nan, np.nan])

    predicted_damages_onethird = data['D_CliExp_raw_onethird_quantile'] * vul_func
    predicted_damages_onethird = predicted_damages_onethird.replace([-np.inf, np.inf],
                                                                    [np.nan, np.nan])

    predicted_damages_twothird = data['D_CliExp_raw_twothird_quantile'] * vul_func
    predicted_damages_twothird = predicted_damages_twothird.replace([-np.inf, np.inf],
                                                                    [np.nan, np.nan])

    return predicted_damages, predicted_damages_onethird, predicted_damages_twothird


def adjust_dam_linear(data, trend_info):
    """
    This function adjusts modeled damages for vulnerability changes using
    the paramter from the sen-slope
    ----------
    data : DataFrame
        DataFrame with modeled damages
    trend_info: np.array
        parameters from the TheilSenSlope estimation


    Returns
    -------
    np.arrays
        damages after accounting for vulnerability, including onethird and two-
        third model quantiles

    """
    vul_func = np.arange(len(data.loc[data['Year']>1979,'D_CliExp_raw']))*trend_info[0]+ trend_info[1]
    vul_func = vul_func/vul_func.max()

    prediction = data['D_CliExp_raw'] * vul_func

    return prediction


def total_damages(data):
    """
    Sums up annual damages to total damages
    ----------
    data : DataFrame
        DataFrame with modeled damages

    Returns
    -------
    total_oberved_damages
    total_model_damages
    total_pred_damages

    """
    total_oberved_damages = np.sum(data['natcat_flood_damages_2005_CPI'].
                                   where(pd.notna(data['D_CliExp_raw'])))
    total_model_damages = np.sum(data['D_CliExp_raw'].
                             where(pd.notna(data['natcat_flood_damages_2005_CPI'])))
    total_pred_damages = np.sum(data['D_Full_raw'].
                               where(pd.notna(data['natcat_flood_damages_2005_CPI'])))

    return total_oberved_damages, total_model_damages, total_pred_damages


def get_explained_variance(reg):
    """
    Calculate explained variance from deviance and null_deviance,
    included in regression result.
    This function is not used in the paper output!
    ----------
    data : DataFrame
        output from regression

    Returns
    -------
    explained variance

    """
    dev_const = reg.null_deviance
    dev_full = reg.deviance

    return 1-(dev_full/dev_const)


def arange_fit_info(region,
                    p_corr_mod, p_corr_pred, p_corr_clim,
                    tot_obs_dam, annual_mean, annual_std, vul_mean):
    """Prepares dataframe for file output
    ----------
    region : string
         shortage of region
    p_corr_mod : CorrelationObject
         pearson correlation coefficient (model/observed)
    p_corr_pred : CorrelationObject
         pearson correlation coefficient (vulnerability adjusted
                                         model/observed)
    tot_obs_dam : float
         total observed damages 1980-2010
    annual_mean : float
         annual mean observed damages 1980-2010
    annual_std : float
         std. deviation in observed damages 1980-2010
    vul_mean : float
         mean vulnerability
    Returns
    -------
    DataFrame
        all output variables for one region

    """

    table1 = pd.DataFrame(data={'Region': region,
                                'R2_D_CliExp_D_Obs':
                                    p_corr_mod[0, 1] * p_corr_mod[0, 1]*100,
                                'R2_D_Full_D_Obs':
                                    p_corr_pred[0, 1] * p_corr_pred[0, 1]*100,
                                'R2_D_1980_D_Obs':
                                    p_corr_clim[0, 1] * p_corr_clim[0, 1]*100,
                                'total_observed_damages': tot_obs_dam,
                                'annual_mean_damage_obs': annual_mean,
                                'std_dev_obs': annual_std,
                                'mean_vulnerability': vul_mean
                                }, index=[0])

    return table1


def add_global(time_series):
    """
    Aggregates global data to provide GLB as additional region

    Parameters
    ----------
    time_series : DataFrame
        Time series of all damages

    Returns
    -------
    time_series : DataFrame
        Damage time series including GLB

    """
    years = np.arange(1971, 2011)
    for yr in years:

        imp = time_series.loc[(time_series['Year'] == yr),
                              'D_CliExp_raw'].sum()
        imp_1980 = time_series.loc[(time_series['Year'] == yr),
                                   'D_1980_raw'].sum()
        imp_2010 = time_series.loc[(time_series['Year'] == yr),
                                   'D_2010_raw'].sum()

        nat_cat = time_series.loc[(time_series['Year'] == yr),
                                  'natcat_flood_damages_2005_CPI'].sum()


        imp_1_3rd = time_series.loc[(time_series['Year'] == yr),
                                    'D_CliExp_raw_onethird_quantile'].sum()

        imp_2_3rd = time_series.loc[(time_series['Year'] == yr),
                                    'D_CliExp_raw_twothird_quantile'].sum()

        time_series = time_series.append({'Year': yr,
                                          'Region': 'GLB',
                                          'D_CliExp_raw': imp,
                                          'D_1980_raw': imp_1980,
                                          'D_2010_raw': imp_2010,
                                          'natcat_flood_damages_2005_CPI':
                                              nat_cat,
                                          'D_CliExp_raw_onethird_quantile':
                                              imp_1_3rd,
                                          'D_CliExp_raw_twothird_quantile':
                                              imp_2_3rd,
                                           }, ignore_index=True)
    return time_series


def vul_fit(dataFrame, rm_columns=None):
    """
    Wrapper function for vulnerability estimation and correlation

    Parameters
    ----------
    dataFrame : DataFrame
        DataFrame containing full time series
    log_char : string
        Aplling logarithmic metrics
    x_dep : TYPE
        variable used for fit time or GDP (not relevant in current version)
    rm_columns : string list, optional
        columns selected for running-mean consideration

    Returns
    -------
    region_data : DataFrame
        All damage Time series including those with vulnerability estimates
    fit_data : DataFrame
        Other metrics, such as correlation, total damages...

    """

    region_data = pd.DataFrame()
    fit_data = pd.DataFrame()

    add_rem_c = 0

    for i, test_region in enumerate(test_regions):

        DATA_region = dataFrame[(dataFrame['Region'] == test_region) &
                                (dataFrame['Year'] < 2011) &
                                (dataFrame['Year'] > 1970)]
        DATA_region = DATA_region.reset_index()

        # get vulnerability ratios

        DATA_region.loc[DATA_region['Year'] > 1979, 'ratios_D_Obs_D_CliExp'] = \
            DATA_region.loc[DATA_region['Year'] > 1979,
                            'natcat_flood_damages_2005_CPI'] / \
            DATA_region.loc[DATA_region['Year'] > 1979, 'D_CliExp_raw']
        DATA_region['ratios_D_Obs_D_CliExp'] = DATA_region['ratios_D_Obs_D_CliExp'].replace([-np.inf, np.inf, 0.0],
                                                              [np.nan, np.nan, np.nan])
        lowctf, upctf = calc_cutoff(DATA_region.loc[DATA_region['Year'] > 1979, 'ratios_D_Obs_D_CliExp'])

        DATA_region.loc[(DATA_region['ratios_D_Obs_D_CliExp'] > upctf),
                        'ratios_D_Obs_D_CliExp'] = np.nan

        # remove events that were classified as outliers in the full region

        add_rem_regions = ['EUR', 'CAS', 'OCE']
        add_rem_events = [[2002, 2007], [2002], [1980, 1985, 1986, 1988, 1989, 2010]]

        if test_region in add_rem_regions:
            DATA_region.loc[DATA_region['Year'].isin(add_rem_events[add_rem_c]),
                            'ratios_D_Obs_D_CliExp'] = np.nan
            add_rem_c += 1
        # count nan evnts
        nan_ev = DATA_region.loc[DATA_region['Year'] > 1979, 'ratios_D_Obs_D_CliExp'].isna().sum()

        vul_func11yr, ssa_5 = vul_func(DATA_region.loc[DATA_region['Year'] > 1979,
                                                       ['ratios_D_Obs_D_CliExp']])

        pred_dam, pred_1thrd, pred_2thrd = \
            adjust_dam(DATA_region[DATA_region['Year'] > 1979], ssa_5)

        pred_dam_mova, pred_1thrd_mova, pred_2thrd_mova = \
            adjust_dam(DATA_region[DATA_region['Year'] > 1979], vul_func11yr)

        DATA_region.loc[DATA_region['Year'] > 1979, 'D_Full_raw'] = pred_dam
        DATA_region.loc[DATA_region['Year'] > 1979, 'D_Full_raw_1thrd_quantile'] = pred_1thrd
        DATA_region.loc[DATA_region['Year'] > 1979, 'D_Full_raw_2thrd_quantile'] = pred_2thrd
        DATA_region.loc[DATA_region['Year'] > 1979, 'vul_func11yr'] = vul_func11yr
        DATA_region.loc[DATA_region['Year'] > 1979, 'vulnerability_function'] = ssa_5

        tot_obs_dam, tot_mod_dam, tot_pred_dam = total_damages(DATA_region[DATA_region['Year'] > 1979])

        annual_mean = DATA_region.loc[DATA_region['Year'] > 1979,
                                              'natcat_flood_damages_2005_CPI'].mean()

        annual_std = DATA_region.loc[DATA_region['Year'] > 1979,
                                             'natcat_flood_damages_2005_CPI'].std()

        vul_mean = DATA_region.loc[DATA_region['Year'] > 1979, 'ratios_D_Obs_D_CliExp'].mean()

        p_corr_pred = pears_corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                                     'natcat_flood_damages_2005_CPI'],
                                     DATA_region.loc[DATA_region['Year'] > 1979,
                                                     'D_Full_raw'],
                                     use_log=False)

        p_corr_mod = pears_corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                                    'natcat_flood_damages_2005_CPI'],
                                    DATA_region.loc[DATA_region['Year'] > 1979,
                                                    'D_CliExp_raw'],
                                    use_log=False)

        p_corr_clim = pears_corr_obs(DATA_region.loc[DATA_region['Year'] > 1979,
                                     'natcat_flood_damages_2005_CPI'],
                                     DATA_region.loc[DATA_region['Year'] > 1979,
                                     'D_1980_raw'],
                                     use_log=False)

        fit_info = arange_fit_info(test_region,
                                   p_corr_mod, p_corr_pred, p_corr_clim,
                                   tot_obs_dam, annual_mean, annual_std, vul_mean)

        region_data = region_data.append(DATA_region, ignore_index=True)
        fit_data = fit_data.append(fit_info, ignore_index=True)

    return region_data, fit_data


DATA = pd.read_csv('../../../data/reconstruction/model_median_regions.csv')

region_names = {'NAM': 'North America',
                'LAM': 'Central America',
                'EUR': 'Europe',
                'NAF': 'North Africa + Middle East',
                'SSA': 'SSA + Southern Africa',
                'CAS': 'Central Asia + Eastern Europe',
                'SEA': 'Southern Asia + South-East Asia',
                'EAS': 'Eastern Asia',
                'OCE': 'Oceania',
                'GLB': 'Global'}

test_regions = list(region_names)


full_ts = add_global(DATA)

region_data_ts, fit_data = vul_fit(full_ts)


region_data_ts.to_csv('../../../data/reconstruction/vulnerability_adjustment_TimeSeries_regions.csv', index=False)

fit_data.to_csv('../../../data/reconstruction/vulnerability_adjustment_MetaData_regions.csv', index=False)

